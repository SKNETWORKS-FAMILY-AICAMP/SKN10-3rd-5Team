from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.memory import ChatMessageHistory
import pandas as pd
from langchain_openai import ChatOpenAI
from langchain_groq import ChatGroq
from langchain_community.retrievers import TavilySearchAPIRetriever
import streamlit as st

# ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì„ ì „ì—­ ë³€ìˆ˜ë¡œ ì €ì¥
original_df = pd.read_csv("./etl/rag/dataset/recipe_data.csv")

# ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ë° ë¦¬íŠ¸ë¦¬ë²„ ìƒì„±
def get_retriever():
    embedding = OpenAIEmbeddings()
    vectorstore = FAISS.load_local(
        "./etl/rag/dataset/recipe_faiss", 
        embeddings=embedding,
        allow_dangerous_deserialization=True
    )
    return vectorstore.as_retriever()

# retrieverì—ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ë‚˜ì˜¨ í›„, ì¡°ë¦¬ìˆœì„œë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_cooking_steps(retrieved_docs):
    enhanced_docs = []
    for doc in retrieved_docs:
        # ì¡°ë¦¬ìˆœì„œê°€ ì´ë¯¸ í¬í•¨ëœ ê²½ìš° ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì¡°ë¦¬ìˆœì„œ ì´ì „ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        content = doc.page_content
        if "ì¡°ë¦¬ìˆœì„œ:" in content:
            content = content.split("ì¡°ë¦¬ìˆœì„œ:")[0].strip()
        # ë ˆì‹œí”¼ID ì¶”ì¶œ
        recipe_ID = None
        content_lines = content.split('\n')
        for line in content_lines:
            if "ë ˆì‹œí”¼ID" in line:
                recipe_ID = line.split(': ')[1].strip()
                break
        if recipe_ID:
            # ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì—ì„œ í•´ë‹¹ ë ˆì‹œí”¼IDì˜ ì¡°ë¦¬ìˆœì„œ ì°¾ê¸°
            recipe_row = original_df[original_df['ë ˆì‹œí”¼ID'] == int(recipe_ID)]
            if not recipe_row.empty and 'ì¡°ë¦¬ìˆœì„œ' in recipe_row.columns:
                cooking_steps = recipe_row['ì¡°ë¦¬ìˆœì„œ'].values[0]
                # ê¸°ì¡´ ë¬¸ì„œ ë‚´ìš©(ì¡°ë¦¬ìˆœì„œ ì œì™¸)ì— ì¡°ë¦¬ìˆœì„œ ì¶”ê°€
                enhanced_content = content + f"\nì¡°ë¦¬ìˆœì„œ: {cooking_steps}"
                doc.page_content = enhanced_content
            else:
                doc.page_content = content
        enhanced_docs.append(doc)
    return enhanced_docs

# ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± í”„ë¡¬í”„íŠ¸
def get_multi_query_chain(llm_gpt):
    multi_query_prompt = ChatPromptTemplate.from_messages([
        ("system", """You are an AI language model assistant.
            Your task is to generate three different versions of the given user question to retrieve relevant documents from a vector database.
            By generating multiple perspectives on the user question, your goal is to help the user overcome some of the limitations of the distance-based similarity search.
            Your response should be a list of values separated by new lines, eg: `foo\nbar\nbaz\n`
            Answer in Korean."""),
        ("human", "{question}")
    ])
    
    multi_query_chain = (
        {"question": RunnablePassthrough()}
        | multi_query_prompt
        | llm_gpt
        | StrOutputParser()
    )
    return multi_query_chain

def filter_relevant_docs(docs, query):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤."""
    if not docs:
        return []
    print("í•„í„°ë˜ê¸° ì „:", len(docs))
    # í•„í„°ë§ì„ ìœ„í•œ LLM ì´ˆê¸°í™” (ê°™ì€ ëª¨ë¸ ì¬ì‚¬ìš© ê°€ëŠ¥)
    # llm = ChatGroq(model_name="gemma2-9b-it")
    llm_gpt = ChatOpenAI(
                model_name="gpt-4o-mini",
                temperature=0.5,
                max_tokens=300,
            )
    filtered_docs = []
    
    for doc in docs:
        # ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
        relevance_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
            ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë ˆì‹œí”¼ ë¬¸ì„œê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
            ë¬¸ì„œê°€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ë˜ëŠ” ì˜ë„ì™€ ê´€ë ¨ì´ ìˆëŠ”ì§€ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.
            ê´€ë ¨ì„±ì´ ë†’ìœ¼ë©´ 'ì°¸'ë§Œ ë‹µë³€í•˜ê³ , ê´€ë ¨ì„±ì´ ë‚®ê±°ë‚˜ ì—†ìœ¼ë©´ 'ê±°ì§“'ë§Œ ë‹µë³€í•˜ì„¸ìš”."""),
            ("human", f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nê²€ìƒ‰ëœ ë¬¸ì„œ: {doc.page_content}\n\nì´ ë¬¸ì„œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‚˜ìš”? 'ì°¸' ë˜ëŠ” 'ê±°ì§“'ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.")
        ])
        
        # ê´€ë ¨ì„± í‰ê°€ ì‹¤í–‰
        try:
            response = relevance_prompt | llm_gpt | StrOutputParser()
            result = response.invoke({}).strip().lower()
            
            # 'ì°¸'ì¸ ê²½ìš°ì—ë§Œ í•„í„°ë§ëœ ë¬¸ì„œ ëª©ë¡ì— ì¶”ê°€
            if result == 'ì°¸':
                filtered_docs.append(doc)
        except Exception as e:
            # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ (í•„í„°ë§ ì‹¤íŒ¨í•˜ë”ë¼ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” ì œê³µ)
            print(f"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    print("í•„í„°ë˜ê¸° í›„:", len(filtered_docs))
    return filtered_docs

# ë‹¤ì¤‘ ê²€ìƒ‰ ì‹¤í–‰
def retrieve_with_steps(multi_queries, original_query):
    retriever = get_retriever()
    all_docs = []
    seen_docs = set()
    
    # ê° ì¿¼ë¦¬ì— ëŒ€í•´ ê²€ìƒ‰ ì‹¤í–‰
    for q in multi_queries:
        if q.strip():  # ë¹„ì–´ìˆì§€ ì•Šì€ ì¿¼ë¦¬ì— ëŒ€í•´ì„œë§Œ ì‹¤í–‰
            docs = retriever.invoke(q)
            for doc in docs[:1]:
                if doc.page_content not in seen_docs:
                    seen_docs.add(doc.page_content)
                    doc.page_content = doc.page_content + "\nì¶œì²˜: " + "https://www.10000recipe.com/recipe/" + doc.page_content.split("ì‹œí”¼ID: ")[1] + "\n----------------------\n"
                    all_docs.append(doc)
    
    # ê²€ìƒ‰ ê²°ê³¼ì— ì¡°ë¦¬ìˆœì„œ ì¶”ê°€
    enhanced_docs = add_cooking_steps(all_docs)

    # LLMì„ ì‚¬ìš©í•˜ì—¬ ê´€ë ¨ì„± í‰ê°€ ë° í•„í„°ë§
    enhanced_docs = filter_relevant_docs(enhanced_docs, original_query)

    # ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° Tavily ê²€ìƒ‰ ìˆ˜í–‰
    if len(enhanced_docs) == 0:
        st.toast("ì¸í„°ë„·ì„ ê²€ìƒ‰í•˜ëŠ” ì¤‘...", icon="ğŸ‘¨â€ğŸ³")
        print("ë‚´ë¶€ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: Tavily ì™¸ë¶€ ê²€ìƒ‰ ì‹¤í–‰")
        try:
            # Tavily ê²€ìƒ‰ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
            tavily_retriever = TavilySearchAPIRetriever(k=3)
            external_docs = []
            
            # ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ Tavily ê²€ìƒ‰ ì‹¤í–‰
            for q in multi_queries:
                if q.strip():
                    try:
                        docs = tavily_retriever.invoke(q)
                        for doc in docs:
                            if doc.page_content not in seen_docs:
                                seen_docs.add(doc.page_content)
                                content = doc.page_content
                                source = doc.metadata["source"]
                                doc.page_content = content + "\nì¶œì²˜: " + source + "\n----------------------\n"
                                external_docs.append(doc)
                    except Exception as e:
                        print(f"Tavily ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ì™¸ë¶€ ë¬¸ì„œë„ ê´€ë ¨ì„± í•„í„°ë§ ì ìš©
            enhanced_docs = filter_relevant_docs(external_docs, original_query)
            print(f"Tavily ê²€ìƒ‰ ê²°ê³¼: {len(enhanced_docs)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
        except Exception as e:
            print(f"Tavily ê²€ìƒ‰ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ë¬¸ì„œ ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹˜ê¸°
    context_text = "\n\n---\n\n".join([doc.page_content for doc in enhanced_docs])
    return context_text

# message_historyë¥¼ langchain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
def convert_messages(message_history):
    chat_history = []
    for msg in message_history:
        if msg["role"] == "system":
            chat_history.append(SystemMessage(content=msg["content"]))
        elif msg["role"] == "user":
            chat_history.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            chat_history.append(AIMessage(content=msg["content"]))
    return chat_history

# ëŒ€í™” ìš”ì•½ì„ ìœ„í•œ í•¨ìˆ˜ ì¶”ê°€
def summarize_conversation(chat_history, llm):
    # ìš”ì•½ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì‘ì„±
    summarize_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ëŒ€í™” ë‚´ìš©ì„ ëª…í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì œê³µëœ ëŒ€í™” ê¸°ë¡ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”:
        1. ì‚¬ìš©ìê°€ ì°¾ê³  ìˆëŠ” ë ˆì‹œí”¼ ìœ í˜•
        2. ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì¬ë£Œ, ì¢‹ì•„í•˜ëŠ” ì¬ë£Œ
        3. ì‚¬ìš©ìê°€ ì‹«ì–´í•˜ëŠ” ì¬ë£Œ
        4. íŠ¹ë³„í•œ ì œì•½ ì¡°ê±´(ì¡°ë¦¬ ì‹œê°„, ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ë“±)
        
        í•µì‹¬ì ì¸ ì •ë³´ë§Œ í¬í•¨í•˜ì—¬ 200ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”."""),
        ("human", "ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•´ì£¼ì„¸ìš”:\n{conversation}")
    ])
    
    # ëŒ€í™” ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    conversation_text = ""
    for msg in chat_history:
        if isinstance(msg, HumanMessage):
            conversation_text += f"ì‚¬ìš©ì: {msg.content}\n"
    
    # ìš”ì•½ ì‹¤í–‰
    summarize_chain = summarize_prompt | llm | StrOutputParser()
    try:
        summary = summarize_chain.invoke({"conversation": conversation_text})
        return summary
    except Exception as e:
        print(f"ëŒ€í™” ìš”ì•½ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return ""

# ì „ì—­ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ ì¶”ê°€
chat_histories = {}
# ì´ì „ ê²€ìƒ‰ ê²°ê³¼ì™€ ëŒ€í™” ìš”ì•½ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
past_contexts = {}
conversation_summaries = {}

# ìµœì‹  ëŒ€í™” ìš”ì•½ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
def get_updated_summary(input_dict):
    session_id = input_dict.get("session_id", "default")
    return conversation_summaries.get(session_id, "ì•„ì§ ëŒ€í™” ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")

# ë ˆì‹œí”¼ RAG ì²´ì¸ ìƒì„±
def create_rag_chain(llm_model, cooking_time=None, cooking_tools=None):
    # LLM ì´ˆê¸°í™”
    llm = llm_model
    llm_gpt = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.7,
        max_tokens=300
    )
    
    # ë‹¤ì¤‘ ì¿¼ë¦¬ ì²´ì¸
    multi_query_chain = get_multi_query_chain(llm_gpt)
    
    # ì¡°ë¦¬ ì‹œê°„ê³¼ ë„êµ¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
    system_message = """
        You are tasked with creating a recipe and detailed cooking instructions based on the ingredients provided.
        The recipe should:

        - Recommend a specific traditional Korean dish that uses the given ingredients.
        - Include a brief introduction to the dish.
        - List all ingredients with accurate amounts (e.g., grams, ml, pieces).
        - List all required cooking tools.
        - Provide step-by-step cooking instructions in numbered format (1, 2, 3, ...).
        - Clearly state the cooking time and tools used in each step.
        - Use traditional Korean cooking techniques (e.g., stir-frying, simmering, steaming).
        - Ensure the instructions are very detailed and easy to follow.
        - Utilize information from the previous context (past_contexts) in the response.
        - Refer to the conversation summary (conversation_summary) to understand the user's overall requirements and context.

        **# Example Format**:
        ìš”ë¦¬ëª…: [ìš”ë¦¬ëª…]
        ê°„ë‹¨ ì„¤ëª…: [ê°„ë‹¨í•œ ìš”ë¦¬ ì„¤ëª…]
        í•„ìš”í•œ ì¬ë£Œ:
        - [ì¬ë£Œ 1]
        - [ì¬ë£Œ 2]

        í•„ìš”í•œ ì¡°ë¦¬ë„êµ¬:
        - [ì¡°ë¦¬ ë„êµ¬ 1]
        - [ì¡°ë¦¬ ë„êµ¬ 2]

        ì¡°ë¦¬ ìˆœì„œ:
        1. [ì¡°ë¦¬ ìˆœì„œ 1]
        2. [ì¡°ë¦¬ ìˆœì„œ 2]
        3. [ì¡°ë¦¬ ìˆœì„œ 3]
        4. [ì¡°ë¦¬ ìˆœì„œ 4]
        
        ì¶œì²˜:

        **Please ensure your response strictly follows the Example Format. Additionally, the response must be in Korean.**
        **ë°˜ë“œì‹œ ì£¼ì–´ì§„ ë¬¸ì„œì˜ ì¶œì²˜ë¥¼ í¬í•¨í•˜ì„¸ìš”**
    """

    # ì¡°ë¦¬ ì‹œê°„ ì œì•½ ì¶”ê°€
    if cooking_time:
        system_message += f"\n#ì¡°ë¦¬ ì‹œê°„: {cooking_time}"
    
    # ì¡°ë¦¬ ë„êµ¬ ì œì•½ ì¶”ê°€
    if cooking_tools and len(cooking_tools) > 0:
        system_message += f"\n#ì‚¬ìš© ê°€ëŠ¥í•œ ì¡°ë¦¬ ë„êµ¬: {', '.join(cooking_tools)}"
    
    # ì´ì „ ì»¨í…ìŠ¤íŠ¸ì™€ ëŒ€í™” ìš”ì•½ì„ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("placeholder", "{chat_history}"),
        ("human", """
        ì´ì „ ëŒ€í™” ìš”ì•½(conversation_summary): {conversation_summary}\n
        ì´ì „ ë¬¸ë§¥(past_contexts): {past_contexts}\n
        í˜„ì¬ ë¬¸ë§¥(Context): {context}\n
        ì§ˆë¬¸: {question}\n
        
        ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ **ë°˜ë“œì‹œ ì¡°ë¦¬ìˆœì„œ, ë„êµ¬, ì¡°ë¦¬ ì‹œê°„, ì¬ë£Œë¥¼ ë‹¨ê³„ë³„ë¡œ í¬í•¨í•˜ì—¬ í•œêµ­ì–´ë¡œ ì‘ì„±**í•´ ì£¼ì„¸ìš”.
        """)
    ])
    
    # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
    def retrieve_context(input_dict):
        session_id = input_dict.get("session_id", "default")
        current_query = input_dict["question"]
        chat_history = input_dict.get("chat_history", [])
        # ì„ì‹œ ëŒ€í™” ê¸°ë¡ì— í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        temp_chat_history = chat_history.copy()
        temp_chat_history.append(HumanMessage(content=current_query))
        
        # ëŒ€í™” ìš”ì•½ ìƒì„±/ì—…ë°ì´íŠ¸ (í˜„ì¬ ì§ˆë¬¸ í¬í•¨)
        if len(temp_chat_history) > 0:
            conversation_summaries[session_id] = summarize_conversation(temp_chat_history, llm)
        print("conversation_summaries:", conversation_summaries)
        # ëŒ€í™” ê¸°ë¡ì—ì„œ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ì¶œ
        user_questions = []
        
        # ëŒ€í™” ê¸°ë¡ì—ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ì¶œ
        for message in chat_history:
            if isinstance(message, HumanMessage):
                user_questions.append(message.content)
        
        # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
        user_questions.append(current_query)
        
        # ëª¨ë“  ì‚¬ìš©ì ì§ˆë¬¸ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©
        combined_query = "\n- ".join(user_questions)
        
        # ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± í›„ ê°ê°ì— ëŒ€í•´ ê²€ìƒ‰ ì‹¤í–‰
        multi_queries = multi_query_chain.invoke(current_query).strip().split("\n")
        
        # ê²€ìƒ‰ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        current_context = retrieve_with_steps(multi_queries, combined_query)
        
        # ì„¸ì…˜ë³„ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
        if session_id not in past_contexts:
            past_contexts[session_id] = []
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì´ì „ ì»¨í…ìŠ¤íŠ¸ ëª©ë¡ì— ì¶”ê°€ (ë„ˆë¬´ ê¸¸ì–´ì§€ì§€ ì•Šê²Œ ìµœê·¼ 2ê°œë§Œ ìœ ì§€)
        past_contexts[session_id].append(current_context)
        if len(past_contexts[session_id]) > 2:
            past_contexts[session_id] = past_contexts[session_id][-2:]
        
        return current_context
    
    # ê¸°ë³¸ RAG ì²´ì¸ êµ¬ì„±
    base_chain = (
        {
            "context": RunnableLambda(retrieve_context),
            "question": lambda x: x["question"],
            "chat_history": lambda x: x["chat_history"],
            "past_contexts": lambda x: "\n\n===PREVIOUS CONTEXT===\n\n".join(
                past_contexts.get(x.get("session_id", "default"), [])
            ),
            "conversation_summary": RunnableLambda(get_updated_summary)
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    # ì„¸ì…˜ì— ë”°ë¥¸ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜
    def get_chat_history(session_id):
        if session_id not in chat_histories:
            chat_histories[session_id] = ChatMessageHistory()
        return chat_histories[session_id]
    
    # ëŒ€í™” ê¸°ë¡ì„ ê´€ë¦¬í•˜ëŠ” ì²´ì¸ìœ¼ë¡œ ë³€í™˜
    rag_chain = RunnableWithMessageHistory(
        base_chain,
        get_chat_history,
        input_messages_key="question",
        history_messages_key="chat_history",
    )
    
    return rag_chain