import time
import streamlit as st

from common.history import add_history
from common.display import write_chat
from common.rag import create_rag_chain
from llm.ollama import Provider_Ollama

from langchain_groq import ChatGroq
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# Groq LLM ì´ˆê¸°í™”
groq_llm = ChatGroq(model_name="gemma2-9b-it")  # ë„ˆê°€ ì“°ëŠ” ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥

def is_cooking_related_question_groq(user_input):
    # system_prompt = "ì´ ì§ˆë¬¸ì´ ìš”ë¦¬ ë ˆì‹œí”¼ë¡œ ë¬´ì—‡ì„ ë§Œë“¤ ìˆ˜ ìˆëƒëŠ” ì§ˆë¬¸ì´ë¼ë©´ 'ìš”ë¦¬', ì•„ë‹ˆë©´ 'ì¼ë°˜'ì´ë¼ê³ ë§Œ ì •í™•í•˜ê²Œ í•œ ë‹¨ì–´ë¡œ ëŒ€ë‹µí•´ì¤˜. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ."
    system_prompt = """
      ì´ ì§ˆë¬¸ì´ 'ìš”ë¦¬ ë ˆì‹œí”¼', 'ë ˆì‹œí”¼', ë˜ëŠ” 'ìŒì‹'ê³¼ ê´€ë ¨ëœ ì§ˆë¬¸ì´ë¼ë©´ 'ìš”ë¦¬'ë¼ê³ ë§Œ ì •í™•í•˜ê²Œ í•œ ë‹¨ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
      ë§Œì•½ ìš”ë¦¬ì™€ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'ì¼ë°˜'ì´ë¼ê³ ë§Œ ì •í™•í•˜ê²Œ í•œ ë‹¨ì–´ë¡œ ëŒ€ë‹µí•´ ì£¼ì„¸ìš”.
      ì˜ˆì‹œ: 'ë‹¬ê±€ê³¼ ì–‘íŒŒë¥¼ í™œìš©í•œ ë ˆì‹œí”¼ë¥¼ ì¶”ì²œí•´ì¤˜.' -> 'ìš”ë¦¬'
      """
    messages = [
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_input)
    ]
    response = groq_llm.invoke(messages).content.strip()
    return response == "ìš”ë¦¬"

def get_llm_model():
    # Provider_Ollama ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    provider = Provider_Ollama()

    # 'gemma3_4b_q8_recipe' ëª¨ë¸ì„ ì„ íƒí•˜ì—¬ ChatOllama ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜
    return provider("gemma3_4b_q8_recipe")

def convert_messages(message_history):
    chat_history = []
    for msg in message_history:
        if msg["role"] == "system":
            chat_history.append(SystemMessage(content=msg["content"]))
        elif msg["role"] == "user":
            chat_history.append(HumanMessage(content=msg["content"]))
        elif msg["role"] == "assistant":
            chat_history.append(AIMessage(content=msg["content"]))
    return chat_history

def filter_relevant_respond(respond, query):
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤."""
    # í•„í„°ë§ì„ ìœ„í•œ LLM ì´ˆê¸°í™” (ê°™ì€ ëª¨ë¸ ì¬ì‚¬ìš© ê°€ëŠ¥)
    
    llm_gpt = ChatOpenAI(
                model_name="gpt-4o-mini",
                temperature=0.5,
                max_tokens=300,
              )
    # ê´€ë ¨ì„± í‰ê°€ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸
    relevance_prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ê²€ìƒ‰ ê²°ê³¼ì˜ ê´€ë ¨ì„±ì„ í‰ê°€í•˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ìƒì„±ëœ ë‹µë³€ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì ì ˆí•œ ë‹µë³€ì¸ì§€ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤.
        ë˜í•œ, ë‹µë³€ì€ ë°˜ë“œì‹œ **ì˜ˆì‹œ í˜•ì‹**ì— ë§ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:
        
        **ì˜ˆì‹œ í˜•ì‹**:
        ìš”ë¦¬ëª…: [ìš”ë¦¬ëª…]
        ê°„ë‹¨ ì„¤ëª…: [ê°„ë‹¨í•œ ìš”ë¦¬ ì„¤ëª…]
        í•„ìš”í•œ ì¬ë£Œ:
        - [ì¬ë£Œ 1]
        - [ì¬ë£Œ 2]
        í•„ìš”í•œ ì¡°ë¦¬ ë„êµ¬:
        - [ì¡°ë¦¬ ë„êµ¬ 1]
        - [ì¡°ë¦¬ ë„êµ¬ 2]
        ì¡°ë¦¬ ìˆœì„œ:
        1. [ì¡°ë¦¬ ìˆœì„œ 1]
        2. [ì¡°ë¦¬ ìˆœì„œ 2]
        3. [ì¡°ë¦¬ ìˆœì„œ 3]
        4. [ì¡°ë¦¬ ìˆœì„œ 4]
        
        ë§Œì•½ ìƒì„±ëœ ë‹µë³€ì´ ì´ **ì˜ˆì‹œ í˜•ì‹**ì— ë§ê³  ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‹¤ë©´ 'ì°¸'ì„ ë‹µë³€í•˜ì„¸ìš”.
        ë§Œì•½ ìƒì„±ëœ ë‹µë³€ì´ í˜•ì‹ì— ë§ì§€ ì•Šê±°ë‚˜ ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ì—†ë‹¤ë©´ 'ê±°ì§“'ë§Œ ë‹µë³€í•˜ì„¸ìš”.
        """),
        ("human", f"ì‚¬ìš©ì ì§ˆë¬¸: {query}\n\nìƒì„±ëœ ë‹µë³€: {respond}\n\nì´ ë‹µë³€ì€ ì˜ˆì‹œ í˜•ì‹ì— ë§ê³  ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆë‚˜ìš”? 'ì°¸' ë˜ëŠ” 'ê±°ì§“'ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.")
    ])
    
    # ê´€ë ¨ì„± í‰ê°€ ì‹¤í–‰
    try:
        response = relevance_prompt | llm_gpt | StrOutputParser()
        result = response.invoke({}).strip().lower()
        
        # 'ì°¸'ì¸ ê²½ìš°ì—ë§Œ í•„í„°ë§ëœ ë¬¸ì„œ ëª©ë¡ì— ì¶”ê°€
        if result == 'ì°¸':
            return True
        else:
            return False
    except Exception as e:
        # ì˜¤ë¥˜ ë°œìƒì‹œ ê¸°ë³¸ì ìœ¼ë¡œ í¬í•¨ (í•„í„°ë§ ì‹¤íŒ¨í•˜ë”ë¼ë„ ê²€ìƒ‰ ê²°ê³¼ëŠ” ì œê³µ)
        print(f"ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False


def get_response_from_llm(message_history, cooking_time, cooking_tools, session_id="default", llm_model=None):
  user_message = message_history[-1]["content"]

  if is_cooking_related_question_groq(user_message):
    st.toast("ìš”ë¦¬ ë ˆì‹œí”¼ë¥¼ ìƒê°í•˜ëŠ” ì¤‘...", icon="ğŸ‘¨â€ğŸ³")
    # RAG ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì‹œí”¼ ë‹µë³€ ìƒì„±
    if llm_model is None:
      llm_model = get_llm_model()
    
    # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    prompts = []
    
    for msg in message_history[:-1]:                   # ì‚¬ìš©ìì˜ ë©”ì‹œì§€ëŠ” ì œì™¸í•´ì•¼ í•¨.
      print(msg)
      prompts.append(tuple(msg.values()))
        
    # ë§ˆì§€ë§‰ ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    prompts += [("user", "{user_input}")]
    
    print("prompts", prompts)
    
    # ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    chat_prompt = ChatPromptTemplate.from_messages(prompts)

    # í”„ë¡¬í”„íŠ¸ -> LLM -> ë¬¸ìì—´ íŒŒì„œë¡œ ì´ì–´ì§€ëŠ” ì²´ì¸ ìƒì„±
    chain = chat_prompt | llm_model | StrOutputParser()
    
    # LLMì˜ ì „ì²´ ì‘ë‹µì„ ë°›ìŒ
    response = chain.invoke({"user_input": user_message}).strip()
    print("LLM ì‘ë‹µ:", response)
    # ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ê´€ë ¨ì„± í‰ê°€
    is_relevant = filter_relevant_respond(response, user_message)
    print("ë‹µë³€ ê´€ë ¨ì„± í‰ê°€:", is_relevant)
    # ë‹µë³€ì´ ê´€ë ¨ì„±ì´ ìˆë‹¤ê³  íŒë‹¨ë˜ë©´ ìŠ¤íŠ¸ë¦¼ì²˜ëŸ¼ ì¶œë ¥
    if is_relevant:
      # ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼: ë‹µë³€ì„ í•œ ê¸€ìì”© ì¶œë ¥í•˜ì—¬ "ì‹¤ì‹œê°„" íš¨ê³¼ë¥¼ ì¤€ë‹¤
      for char in response:
        yield char
        time.sleep(0.05)  # ì•½ê°„ì˜ ì§€ì—°ì„ ì£¼ì–´ ìŠ¤íŠ¸ë¦¬ë° íš¨ê³¼ë¥¼ ì¤€ë‹¤
    else:
      st.toast("ë‹¤ë¥¸ ìš”ë¦¬ ë ˆì‹œí”¼ë¥¼ ì°¸ê³ í•˜ëŠ” ì¤‘...", icon="ğŸ‘¨â€ğŸ³")
      rag_chain = create_rag_chain(groq_llm, cooking_time, cooking_tools)

      # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
      for token in rag_chain.stream(
        {"question": user_message},
        config={"configurable": {"session_id": session_id}},
      ):
        yield token
        time.sleep(0.05)

  else:
    st.toast("ì ì ˆí•œ ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘...", icon="ğŸ‘¨â€ğŸ³")
    # âœ… ì¼ë°˜ ì§ˆë¬¸ì¼ ê²½ìš° â†’ Groq GPT ì§ì ‘ ì‘ë‹µ
    messages = [SystemMessage(content="ì¹œì ˆí•œ ìš”ë¦¬ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”.")] + [
      HumanMessage(content=msg["content"]) if msg["role"] == "user" else
      SystemMessage(content=msg["content"]) if msg["role"] == "system" else
      HumanMessage(content=msg["content"])  # assistantë„ HumanMessageì²˜ëŸ¼ ì²˜ë¦¬
      for msg in message_history if msg["role"] != "system"
    ]

    for chunk in groq_llm.stream(messages):
      if hasattr(chunk, "content") and chunk.content:
        yield chunk.content
        time.sleep(0.05)

def ask(question, message_history, cooking_time=None, cooking_tools=None, llm_model=None):
  if len(message_history) == 0:
    # ìµœì´ˆ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    message_history.append({
        "role": "system", 
        "content": """
        ë‹¹ì‹ ì€ ìš”ë¦¬ ë ˆì‹œí”¼ì™€ ì¡°ë¦¬ ë°©ë²•ì„ ì œê³µí•˜ëŠ” AIì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì¬ë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ í˜•ì‹ì— ë§ëŠ” ë ˆì‹œí”¼ì™€ ì¡°ë¦¬ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. 
        ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì— ë§ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:

        #Example Format:
        (ê°„ë‹¨í•œ ìš”ë¦¬ ì„¤ëª…)

        | í•­ëª©          | ë‚´ìš©                                      |
        | ------------- | ----------------------------------------- |
        | **ìš”ë¦¬ëª…**    | [ìš”ë¦¬ëª… 1]                                  |
        | **ê°„ë‹¨ ì„¤ëª…** | [ê°„ë‹¨í•œ ìš”ë¦¬ ì„¤ëª… 1]                        |
        | **í•„ìš”í•œ ì¬ë£Œ** | [ì¬ë£Œ 1], [ì¬ë£Œ 2], [ì¬ë£Œ 3] (ì˜ˆ: ê·¸ë¨, ml, ê°œ ë‹¨ìœ„) |
        | **í•„ìš”í•œ ì¡°ë¦¬ ë„êµ¬** | [ì¡°ë¦¬ ë„êµ¬ 1], [ì¡°ë¦¬ ë„êµ¬ 2]              |
        | **ì¡°ë¦¬ ìˆœì„œ** | 1. [ì¡°ë¦¬ ìˆœì„œ 1] <br> 2. [ì¡°ë¦¬ ìˆœì„œ 2] <br> 3. [ì¡°ë¦¬ ìˆœì„œ 3] <br> 4. [ì¡°ë¦¬ ìˆœì„œ 4] |

        **ë‹µë³€ì€ ë°˜ë“œì‹œ ìœ„ì˜ ì˜ˆì‹œ í˜•ì‹ì„ ì§€ì¼œì•¼ í•˜ë©°, ëª¨ë“  ë‹µë³€ì€ í•œê¸€ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ì¤„ê¸€ í˜•ì‹ì€ ì ˆëŒ€ í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.**
        """
    })




  # ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€ ë° ì¦‰ì‹œ í‘œì‹œ
  message_history = add_history(message_history, role="user", content=question)
  write_chat(role="user", message=message_history[-1]["content"])

  # ì„¸ì…˜ ID ìƒì„± (ì‚¬ìš©ìë§ˆë‹¤ ê³ ìœ í•œ ID ì‚¬ìš©)
  import uuid
  if "session_id" not in st.session_state:
    st.session_state.session_id = str(uuid.uuid4())
  
  # LLM ë‹µë³€ ì¦‰ì‹œ í‘œì‹œ ë° ì¶”ê°€
  response = write_chat(
    role="assistant",
    message=get_response_from_llm(message_history, cooking_time, cooking_tools, st.session_state.session_id, llm_model)  # ì„¸ì…˜ ID ì „ë‹¬
  )
  message_history = add_history(message_history, role="assistant", content=response)

  return message_history
